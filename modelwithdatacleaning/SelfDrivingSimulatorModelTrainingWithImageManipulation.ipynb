{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c673959",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow\n",
    "#!conda install tensorflow=${version} -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59b6bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install these packages if not installed.  For tensorflow, the Pip method is preferred but I had trouble on Mac and\n",
    "#   had to use conda.  Only need to do Pip or COnda and only if not installed.\n",
    "\n",
    "#!pip install google-api-python-client google-auth-httplib2 google-auth-oauthlib \"\"\"***Use this line if images are in google drive\"\"\"\"\"\"\n",
    "#!pip install opencv-python -y\n",
    "\n",
    "#!pip install tensorflow\n",
    "#!conda install tensorflow=${version} -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "917a661d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-09 20:04:21.481406: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#Import various libraries.  I dont beleive this will work in google colab as colab and openCV don't play well together \n",
    "#as of June 2023.  \n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from tensorflow.python.keras.utils.data_utils import Sequence\n",
    "\n",
    "import cv2\n",
    "from sklearn.model_selection  import train_test_split\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Conv2D,Dense,Flatten,Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import scipy.misc\n",
    "import os\n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "keras = tf.compat.v1.keras #Had to go with a legacy version of Keras because of trouble with the datagenerator\n",
    "Sequence = keras.utils.Sequence\n",
    "\n",
    "\n",
    "\n",
    "from imutils import paths\n",
    "\n",
    "#initial learning rate and dimensions of our images\n",
    "INIT_LR = 1e-4\n",
    "input_shape = (66, 200, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67068f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for datagen.py\n",
    "params = {'dim': (66,200,3),\n",
    "          'batch_size': 32,\n",
    "          'shuffle': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf88b356",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the path of the folder containing your training images\n",
    "\n",
    "path_name=\"/Users/dianevaught/Documents/Jeff/ML_Projects/SelfDrivingSimulation/RoadImages0to57999/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1d3f8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This opens the key file and creates two lists, the angle (target) and the ids which is the file name without jpg\n",
    "\n",
    "angle=[]\n",
    "ids=[]\n",
    "\n",
    "f= open(\"/Users/dianevaught/Documents/Jeff/ML_Projects/SelfDrivingSimulation/Nvidiadatakey - SelfDrivingCarNvidiadatakey0to57999.txt\")                                 #read steering angles from disk and preprocess\n",
    "data = f.read()\n",
    "data = data.split()\n",
    "for i in data:                                      #if the node end with \".jpg\" ignore it. what we need is only angles\n",
    "    if i[-1]=='g':\n",
    "        i = i[:-4]\n",
    "        ids.append(i)\n",
    "    else:\n",
    "        angle.append(float(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c6af3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#train and test set ratio. We create two dictionaries for data batch generator. \n",
    "#partition consists of two lists that holds the train and validation image ids. labels is a dictionary that\n",
    "#hold the angles.\n",
    "\n",
    "\n",
    "ids = list(map(int, ids))\n",
    "partition={'train':ids[:int(len(ids)*.8)],'validation':ids[-int(len(ids)*.2):]}\n",
    "labels={}\n",
    "for i in partition[\"train\"]:\n",
    "    labels[i]=float(angle[i])* scipy.pi / 180\n",
    "for i in partition[\"validation\"]:\n",
    "    labels[i]=float(angle[i])* scipy.pi / 180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "080ef14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the image processing.  It uses various techniques to improve training.  This offered a SIGNIFICANT\n",
    "#improvement of the training data\n",
    "\n",
    "def preProcessing(img):\n",
    "  \n",
    "  #crops image, the goal is to get the road and nothing else.  Will depend on camera placement\n",
    "  img= img[120:235,:,:]\n",
    "\n",
    "  #converts to an image that changes colors but helps the computer see lane lines\n",
    "  img = cv2.cvtColor(img, cv2.COLOR_RGB2YUV)\n",
    "\n",
    "  #blurs the image improving model performance\n",
    "  img =cv2.GaussianBlur(img, (3,3),0)\n",
    "\n",
    "  #resizes the image as this is what Nvidia trained on\n",
    "  img=cv2.resize(img, (200, 66))\n",
    "\n",
    "  #Normalizes image.  Changes color values from 0 to 255 to 0 to 1.  This is to ensure that no\n",
    "  # large numbers become more important on the model\n",
    "  img = img/255\n",
    "\n",
    "  return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b973aaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Generator that provides the images in batches.  This allows for faster training as a number of images can\n",
    "#be process at once, but not so many that the memory fills and crashes the program. More robust computers can \n",
    "#imcrease the batch size to speed up training\n",
    "\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, labels, batch_size=32, dim=(66,200,3), shuffle=True):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp =[self.list_IDs[k] for k in indexes]\n",
    "        \n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' \n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, *self.dim))\n",
    "        y = np.empty(self.batch_size)\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "\n",
    "            # Store sample.  This is the feature data stored in a NUMPY array\n",
    "            image = cv2.imread(\"/Users/dianevaught/Documents/Jeff/ML_Projects/SelfDrivingSimulation/RoadImages0to57999/\"+str(ID)+\".jpg\")     #read images from disk\n",
    "            image=preProcessing(image)\n",
    "\n",
    "            X[i,] = np.asarray(image)\n",
    "\n",
    "            # Store class.  This is the target stored in a NUMPY array\n",
    "            y[i] = np.asarray(float(self.labels[ID]))\n",
    "\n",
    "        return X, y\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a7de57d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generators  This tells the model what pictures to apply the DataGenerator to.\n",
    "training_generator = DataGenerator(partition[\"train\"], labels, **params)\n",
    "validation_generator = DataGenerator(partition[\"validation\"], labels, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3fccd0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#our model is Nvidia Dave-2 which you can find here: https://arxiv.org/pdf/1604.07316.pdf\n",
    "def defineModel():\n",
    "    model = Sequential()\n",
    "\n",
    "    # 5x5 Convolutional layers with stride of 2x2\n",
    "    model.add(Conv2D(24, (5, 5), strides=(2, 2),activation='elu',input_shape=input_shape))\n",
    "    model.add(Conv2D(36, (5, 5), strides=(2, 2),activation='elu'))\n",
    "    model.add(Conv2D(48, (5, 5), strides=(2, 2),activation='elu'))\n",
    "    \n",
    "    # 3x3 Convolutional layers with stride of 1x1\n",
    "    model.add(Conv2D(64, (3, 3),activation='elu'))\n",
    "    model.add(Conv2D(64, (3, 3),activation='elu'))\n",
    "    \n",
    "    # Flatten before passing to the fully connected layers\n",
    "    model.add(Flatten())\n",
    "    # Three fully connected layers\n",
    "    model.add(Dense(100,activation='elu'))\n",
    "    model.add(Dropout(.2))\n",
    "    model.add(Dense(50,activation='elu'))\n",
    "    model.add(Dropout(.2))\n",
    "    model.add(Dense(10,activation='elu'))\n",
    "    model.add(Dropout(.2))\n",
    "    \n",
    "    # Output layer with linear activation \n",
    "    model.add(Dense(1,activation=\"linear\"))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7aa65c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This compiles the model before training\n",
    "model=defineModel()\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "63b8eeb6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1450/1450 - 234s - loss: 0.1755 - val_loss: 0.1253 - 234s/epoch - 162ms/step\n",
      "Epoch 2/10\n",
      "1450/1450 - 234s - loss: 0.1332 - val_loss: 0.1057 - 234s/epoch - 161ms/step\n",
      "Epoch 3/10\n",
      "1450/1450 - 227s - loss: 0.1126 - val_loss: 0.1149 - 227s/epoch - 157ms/step\n",
      "Epoch 4/10\n",
      "1450/1450 - 221s - loss: 0.0974 - val_loss: 0.1888 - 221s/epoch - 152ms/step\n",
      "Epoch 5/10\n",
      "1450/1450 - 259s - loss: 0.0971 - val_loss: 0.2766 - 259s/epoch - 179ms/step\n",
      "Epoch 6/10\n",
      "1450/1450 - 234s - loss: 0.0772 - val_loss: 0.2038 - 234s/epoch - 161ms/step\n",
      "Epoch 7/10\n",
      "1450/1450 - 227s - loss: 0.0759 - val_loss: 0.2604 - 227s/epoch - 156ms/step\n",
      "Epoch 8/10\n",
      "1450/1450 - 232s - loss: 0.0746 - val_loss: 0.1543 - 232s/epoch - 160ms/step\n",
      "Epoch 9/10\n",
      "1450/1450 - 227s - loss: 0.0721 - val_loss: 0.1292 - 227s/epoch - 156ms/step\n",
      "Epoch 10/10\n",
      "1450/1450 - 231s - loss: 0.0715 - val_loss: 0.2529 - 231s/epoch - 160ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f937eba8190>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train the model for 10 epochs\n",
    "\n",
    "\n",
    "model.fit(training_generator,\n",
    "                    epochs=10,   \n",
    "                    validation_data=validation_generator, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d6db87f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://d3e960ad-ac72-4ae3-954e-8dac1dbe99e1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://d3e960ad-ac72-4ae3-954e-8dac1dbe99e1/assets\n"
     ]
    }
   ],
   "source": [
    "# save the model to disk\n",
    "filename = '/Users/dianevaught/Documents/Jeff/ML_Projects/SelfDrivingSimulation/models/finalized_model2.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))\n",
    " \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
